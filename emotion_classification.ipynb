{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m2_J5Kh72zR"
      },
      "source": [
        "# Emotion Classification\n",
        "A transformer Neural Network to perform emotion classification from texts on Kaggle dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmvYn7Fu-jWq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZjEGY-L96-O"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "from sentence_transformers.evaluation import SentenceEvaluator\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import Dataset, Subset\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "from transformers import BertTokenizer\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Union, Tuple, List, Iterable, Dict, Callable\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import json\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iXZucouC60S"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaG58IKRDVUs"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/Intelligent System/Assignment_1/pba1-emotion-classification\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQu1Cvcw7u6-"
      },
      "source": [
        "## Data Preparation & Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqVKzCEw76jH"
      },
      "outputs": [],
      "source": [
        "# Map data files to splits\n",
        "data_files = {'train': 'training.csv', 'validation': 'validation.csv', 'test': 'test.csv'}\n",
        "\n",
        "ds = load_dataset('./dataset', data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3r5uNIJFiV0"
      },
      "outputs": [],
      "source": [
        "ds.shape ## Dataset shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykZizPQpF7qJ"
      },
      "outputs": [],
      "source": [
        "# Calculate the maximum sequence length\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "max_seq_len = reduce(max, [len(tokenizer.tokenize(seq)) for seq in ds['train']['text']])\n",
        "\n",
        "# Number of classes\n",
        "num_class = len(set(ds['train']['label']))\n",
        "\n",
        "print(\"Maximum Sequence Length: \", max_seq_len)\n",
        "print(\"Number of Emotion Classes: \", num_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i42qd9QcSy8-"
      },
      "outputs": [],
      "source": [
        "# Classes: Sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)\n",
        "idx_to_label = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "train_data = ds[\"train\"]\n",
        "eval_data = ds[\"validation\"]\n",
        "test_data = ds[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF04GTWkHjbT"
      },
      "outputs": [],
      "source": [
        "## Create a wordcloud for \"sadness\"\n",
        "sadness_text = \" \".join(train_data.filter(lambda example: example['label'] == 0)['text'])\n",
        "plt.figure(figsize = (15, 10))\n",
        "wordcloud = WordCloud(max_words=200, height= 300, width = 500, background_color=\"black\", colormap= 'viridis').generate(sadness_text)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Sadness\", fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "## Create a wordcloud for \"joy\"\n",
        "joy_text = \" \".join(train_data.filter(lambda example: example['label'] == 1)['text'])\n",
        "plt.figure(figsize = (15, 10))\n",
        "wordcloud = WordCloud(max_words=200, height= 300, width = 500, background_color=\"black\", colormap= 'viridis').generate(joy_text)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Joy\", fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "## Create a wordcloud for \"love\"\n",
        "love_text = \" \".join(train_data.filter(lambda example: example['label'] == 2)['text'])\n",
        "plt.figure(figsize = (15, 10))\n",
        "wordcloud = WordCloud(max_words=200, height= 300, width = 500, background_color=\"black\", colormap= 'viridis').generate(love_text)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Love\", fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "## Create a wordcloud for \"anger\"\n",
        "anger_text = \" \".join(train_data.filter(lambda example: example['label'] == 3)['text'])\n",
        "plt.figure(figsize = (15, 10))\n",
        "wordcloud = WordCloud(max_words=200, height= 300, width = 500, background_color=\"black\", colormap= 'viridis').generate(anger_text)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Anger\", fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "## Create a wordcloud for \"fear\"\n",
        "fear_text = \" \".join(train_data.filter(lambda example: example['label'] == 4)['text'])\n",
        "plt.figure(figsize = (15, 10))\n",
        "wordcloud = WordCloud(max_words=200, height= 300, width = 500, background_color=\"black\", colormap= 'viridis').generate(fear_text)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Fear\", fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "## Create q wordcloud for \"surprise\"\n",
        "surprise_text = \" \".join(train_data.filter(lambda example: example['label'] == 5)['text'])\n",
        "plt.figure(figsize = (15, 10))\n",
        "wordcloud = WordCloud(max_words=200, height= 300, width = 500, background_color=\"black\", colormap= 'viridis').generate(surprise_text)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Surprise\", fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsfPOYL2766f"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ1NOoRE8Ap_"
      },
      "outputs": [],
      "source": [
        "# Define model architecture:\n",
        "# - Base uncased BERT\n",
        "# - Pooling layer\n",
        "# - Fully connected layer (dim=256)\n",
        "# - Fully connected layer (dim=6)\n",
        "\n",
        "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=128)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "fc1_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
        "fc2_model = models.Dense(in_features=256, out_features=num_class, activation_function=nn.Softmax())\n",
        "\n",
        "classification_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, fc1_model, fc2_model])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL0uuu5P_SvK"
      },
      "outputs": [],
      "source": [
        "# Create a class to calculate cross-entropy loss based on softmax outputs\n",
        "class SoftmaxLoss(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model: SentenceTransformer,\n",
        "                 loss_fct: Callable = nn.CrossEntropyLoss()):\n",
        "        super(SoftmaxLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.loss_fct = loss_fct\n",
        "\n",
        "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
        "        output = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
        "        output = torch.squeeze(torch.stack(output))\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fct(output, labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return output, output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNG3IE4p8BRh"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-iZZQeHu55D"
      },
      "outputs": [],
      "source": [
        "## Load the trained model directly to save time of training\n",
        "classification_model = SentenceTransformer(\"./model_pretrained/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGrjB9gf8E9q"
      },
      "outputs": [],
      "source": [
        "### ATTENTION !!!!\n",
        "### Only run this block of code if the model has not yet been trained.\n",
        "### ========================================\n",
        "\n",
        "### ***Uncomment below to train\n",
        "'''\n",
        "## Train the model\n",
        "from sentence_transformers import InputExample, losses, SentencesDataset, evaluation\n",
        "from torch.utils.data import DataLoader\n",
        "from evaluate import evaluator\n",
        "\n",
        "batch_size = 64\n",
        "num_epoch = 32\n",
        "\n",
        "# Define the train examples.\n",
        "train_examples = []\n",
        "for elem in train_data:\n",
        "  train_examples.append(InputExample(texts=[elem['text']], label=elem['label']))\n",
        "\n",
        "# Define your train dataset, the dataloader and the train loss\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
        "train_loss = SoftmaxLoss(model=classification_model)\n",
        "\n",
        "# Define the validation examples.\n",
        "eval_examples = []\n",
        "for elem in eval_data:\n",
        "  eval_examples.append(InputExample(texts=[elem['text']], label=elem['label']))\n",
        "\n",
        "# Evaluate model\n",
        "validation_dataloader = DataLoader(eval_examples, shuffle=True, batch_size=batch_size)\n",
        "label_evaluator = evaluation.LabelAccuracyEvaluator(validation_dataloader, softmax_model=train_loss)\n",
        "\n",
        "\n",
        "# Start to the model\n",
        "classification_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "                         epochs=num_epoch,\n",
        "                         warmup_steps=100,\n",
        "                         evaluator=label_evaluator,\n",
        "                         evaluation_steps=500,\n",
        "                         output_path=\"./model_pretrained\",\n",
        "                         save_best_model=True)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9056Wsg8FHn"
      },
      "source": [
        "## Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "165e7v7v65Ed"
      },
      "source": [
        "Show occurrences of each class in the training data. This can be used to explain the confusion matrix of test data prediction.\n",
        "\n",
        "As we know, more training data is generally better so that a model can learn a more diverse and comprehensive representation of inputs. This explains why the true positive rate or recall is the lowest for `surprise` class (aka 35/66 = 0.53) because the model is only trained with 572 input examples of `surprise` class (the lowest among all classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVkD_wGRD9qW"
      },
      "outputs": [],
      "source": [
        "### Predict test data using the trained model. Use the test data from the original dataset.\n",
        "predictions = classification_model.encode(test_data['text'])\n",
        "prediction_labels = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cu1j1oe8Huo"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
        "\n",
        "## Show the frequency of occurrences for each class label in the training dataset\n",
        "class_counts = Counter(train_data['label'])\n",
        "label_freq_df = pd.DataFrame.from_dict(class_counts, orient='index').sort_index()\n",
        "label_freq_df[\"index\"] = list(idx_to_label.values())\n",
        "label_freq_df = label_freq_df.set_index(\"index\")\n",
        "label_freq_df.plot(kind='bar', legend=False, ax=axes[0], subplots=True)\n",
        "axes[0].bar_label(axes[0].containers[0])\n",
        "axes[0].set_title(\"Occurences of Each Class in Training Data\")\n",
        "\n",
        "## Confusion Matrix\n",
        "# print(list(idx_to_label.values()))\n",
        "ConfusionMatrixDisplay.from_predictions(test_data['label'], prediction_labels, display_labels=list(idx_to_label.values()), ax=axes[1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgLeSHgHCXEt"
      },
      "source": [
        "Classification Report showing precision, recall, f1-score, support and overall accuracy for each class label. Macro average and weighted average of precision, recall and F1 are shown as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQVKK76nDOol"
      },
      "outputs": [],
      "source": [
        "# Support - the number of actual occurrences of the class in the specified dataset.\n",
        "print(classification_report(test_data['label'], prediction_labels, target_names=list(idx_to_label.values())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XSwk8yICh5Q"
      },
      "source": [
        "Show the distribution of softmax probabilities for each class label. This is used to indicate how certain the model makes the prediction of test examples. The value approaching to 1 means that the model is very sure about the emotion conveyed by a test example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGhv9HUyOGg0"
      },
      "outputs": [],
      "source": [
        "prediction_label_scores = np.max(predictions, axis=1)\n",
        "label_score_df = pd.DataFrame({'label': prediction_labels, 'score': prediction_label_scores})\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 8))\n",
        "\n",
        "axes[0, 0].hist(label_score_df[label_score_df['label'] == 0]['score'], bins=50)\n",
        "axes[0, 0].set_title(\"Softmax Value Distribution of Label 0 (Sadness)\")\n",
        "\n",
        "axes[0, 1].hist(label_score_df[label_score_df['label'] == 1]['score'], bins=50)\n",
        "axes[0, 1].set_title(\"Softmax Value Distribution of Label 1 (Joy)\")\n",
        "\n",
        "axes[0, 2].hist(label_score_df[label_score_df['label'] == 2]['score'], bins=50)\n",
        "axes[0, 2].set_title(\"Softmax Value Distribution of Label 2 (Love)\")\n",
        "\n",
        "axes[1, 0].hist(label_score_df[label_score_df['label'] == 3]['score'], bins=50)\n",
        "axes[1, 0].set_title(\"Softmax Value Distribution of Label 4 (Anger)\")\n",
        "\n",
        "axes[1, 1].hist(label_score_df[label_score_df['label'] == 4]['score'], bins=50)\n",
        "axes[1, 1].set_title(\"Softmax Value Distribution of Label 5 (Fear)\")\n",
        "\n",
        "axes[1, 2].hist(label_score_df[label_score_df['label'] == 5]['score'], bins=50)\n",
        "axes[1, 2].set_title(\"Softmax Value Distribution of Label 6 (Surprise)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gXs7ZWDSEe2"
      },
      "outputs": [],
      "source": [
        "# _, bins, _ = plt.hist(test_data_seq_lens, bins=5, range=[0, 75], align='mid')\n",
        "# plt.title(\"Test Data Sequence Length Distribution\")\n",
        "# plt.show()\n",
        "\n",
        "## Sentence length distribution by labels\n",
        "\n",
        "### For TRAINING data\n",
        "train_data_seq_lens = []\n",
        "for label_id in idx_to_label:\n",
        "  # Filter samples with class label `label_id`\n",
        "  filtered_dataset = list(filter(lambda item: item['label'] == label_id, train_data))\n",
        "  train_data_seq_lens.append([len(tokenizer.tokenize(seq['text'])) for seq in filtered_dataset])\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "_, bins, _ = plt.hist(train_data_seq_lens, bins=5, range=[0, 75], stacked=True)\n",
        "\n",
        "plt.legend(idx_to_label.values())\n",
        "plt.title(\"Training Data Sequence Length Distribution\")\n",
        "plt.xlabel(\"Sequence Length\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### For TESTING data\n",
        "test_data_seq_lens = []\n",
        "for label_id in idx_to_label:\n",
        "  # Filter samples with class label `label_id`\n",
        "  filtered_dataset = list(filter(lambda item: item['label'] == label_id, test_data))\n",
        "  test_data_seq_lens.append([len(tokenizer.tokenize(seq['text'])) for seq in filtered_dataset])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "_, bins, _ = plt.hist(test_data_seq_lens, bins=5, range=[0, 75], stacked=True)\n",
        "\n",
        "plt.legend(idx_to_label.values())\n",
        "plt.title(\"Test Data Sequence Length Distribution\")\n",
        "plt.xlabel(\"Sequence Length\")\n",
        "plt.show()\n",
        "\n",
        "bins = [int(a) for a in bins]\n",
        "print(\"Bins:\", bins)\n",
        "\n",
        "# plt.hist(x3, bins, stacked=True, density = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhyp9NWhOnvF"
      },
      "source": [
        "Assess the model performance by `Sequence Length` factor: (0-15, 15-30, 30-45, 45-60, 60-75) to understand the effect of sequence length on the model prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf55YH3rYHQ5"
      },
      "outputs": [],
      "source": [
        "# Assign the correct sequence length group to each text example.\n",
        "len_tuples = []\n",
        "len_labels = []\n",
        "for index in range(len(bins) - 1):\n",
        "  len_tuples.append((bins[index], bins[index+1]))\n",
        "  len_labels.append(\"{:d} <= x < {:d}\".format(bins[index], bins[index+1]))\n",
        "\n",
        "def assign_group_to_len(x):\n",
        "  for index, t in enumerate(len_tuples):\n",
        "    if x >= t[0] and x < t[1]:\n",
        "      return index\n",
        "\n",
        "test_data_seq_lens = [len(tokenizer.tokenize(seq)) for seq in test_data['text']]\n",
        "seq_len_label_df = pd.DataFrame({'len': test_data_seq_lens, 'predicted_label': prediction_labels, 'true_label': test_data['label']})\n",
        "seq_len_label_df[\"len_group\"] = seq_len_label_df[\"len\"].map(assign_group_to_len)\n",
        "\n",
        "\n",
        "## Calculate false positive rate, false negative rate, False Discovery Rate, False Omission Rate\n",
        "false_performance_per_label = seq_len_label_df[[\"len_group\", \"true_label\"]].drop_duplicates()\n",
        "false_performance_per_label[\"TP\"] = np.nan\n",
        "false_performance_per_label[\"FP\"] = np.nan\n",
        "false_performance_per_label[\"TN\"] = np.nan\n",
        "false_performance_per_label[\"FN\"] = np.nan\n",
        "\n",
        "seq_len_group = false_performance_per_label[\"len_group\"].unique()\n",
        "for i in seq_len_group:\n",
        "  for cls in idx_to_label:\n",
        "    filtered_data = seq_len_label_df[seq_len_label_df[\"len_group\"] == i]\n",
        "    row_index = false_performance_per_label[(false_performance_per_label['len_group'] == i) & (false_performance_per_label['true_label'] == cls)].index\n",
        "\n",
        "    if row_index.empty:\n",
        "      new_row = pd.DataFrame({'len_group': i, 'true_label': cls}, index=[0])\n",
        "      false_performance_per_label = pd.concat([false_performance_per_label, new_row], ignore_index = True)\n",
        "\n",
        "    false_performance_per_label.loc[row_index, \"TP\"] = ((filtered_data['predicted_label'] == cls) & (filtered_data['true_label'] == cls)).sum()\n",
        "    false_performance_per_label.loc[row_index, \"FP\"] = ((filtered_data['predicted_label'] == cls) & (filtered_data['true_label'] != cls)).sum()\n",
        "    false_performance_per_label.loc[row_index, \"TN\"] = ((filtered_data['predicted_label'] != cls) & (filtered_data['true_label'] != cls)).sum()\n",
        "    false_performance_per_label.loc[row_index, \"FN\"] = ((filtered_data['predicted_label'] != cls) & (filtered_data['true_label'] == cls)).sum()\n",
        "\n",
        "## False Positive Rate = FP / (FP + TN)\n",
        "## False Negative Rate = FN / (FN + TP)\n",
        "## False Discovery Rate = FP / (FP + TP)\n",
        "## False Omission Rate = FN / (FN + TN)\n",
        "false_performance_per_label[\"FNR\"] = false_performance_per_label[\"FN\"] / (false_performance_per_label[\"FN\"] + false_performance_per_label[\"TP\"])\n",
        "false_performance_per_label[\"FPR\"] = false_performance_per_label[\"FP\"] / (false_performance_per_label[\"FP\"] + false_performance_per_label[\"TN\"])\n",
        "false_performance_per_label[\"FDR\"] = false_performance_per_label[\"FP\"] / (false_performance_per_label[\"FP\"] + false_performance_per_label[\"TP\"])\n",
        "false_performance_per_label[\"FOR\"] = false_performance_per_label[\"FN\"] / (false_performance_per_label[\"FN\"] + false_performance_per_label[\"TN\"])\n",
        "\n",
        "false_performance_per_label = false_performance_per_label.sort_values(['len_group', 'true_label'], ascending = [True, True])\n",
        "# print(false_performance_per_label.head())\n",
        "# print(false_performance_per_label[(false_performance_per_label['len_group'] == 4) & (false_performance_per_label['true_label'] == 0)])\n",
        "\n",
        "\n",
        "### Plot a heatmap to show the rates\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n",
        "\n",
        "## Plot a heatmap of False Positive Rate\n",
        "sns.heatmap(false_performance_per_label[\"FPR\"].values.reshape(len(seq_len_group), len(idx_to_label)), cmap='RdPu', annot=True, ax=axes[0,0],\n",
        "            xticklabels=idx_to_label.values(), yticklabels=len_labels)\n",
        "axes[0,0].set_title(\"False Positive Rate For Each Sequence Length and Class\")\n",
        "axes[0,0].set_xlabel(\"Class\")\n",
        "axes[0,0].set_ylabel(\"Sequence Length of Test Examples, x\")\n",
        "\n",
        "## Plot a heatmap of False Negative Rate\n",
        "sns.heatmap(false_performance_per_label[\"FNR\"].values.reshape(len(seq_len_group), len(idx_to_label)), cmap='RdPu', annot=True, ax=axes[0,1],\n",
        "            xticklabels=idx_to_label.values(), yticklabels=len_labels)\n",
        "axes[0,1].set_title(\"False Negative Rate For Each Sequence Length and Class\")\n",
        "axes[0,1].set_xlabel(\"Class\")\n",
        "axes[0,1].set_ylabel(\"Sequence Length of Test Examples, x\")\n",
        "\n",
        "## Plot a heatmap of False Discovery Rate\n",
        "sns.heatmap(false_performance_per_label[\"FDR\"].values.reshape(len(seq_len_group), len(idx_to_label)), cmap='RdPu', annot=True, ax=axes[1,0],\n",
        "            xticklabels=idx_to_label.values(), yticklabels=len_labels)\n",
        "axes[1,0].set_title(\"False Discovery Rate For Each Sequence Length and Class\")\n",
        "axes[1,0].set_xlabel(\"Class\")\n",
        "axes[1,0].set_ylabel(\"Sequence Length of Test Examples, x\")\n",
        "\n",
        "## Plot a heatmap of False Omission Rate\n",
        "sns.heatmap(false_performance_per_label[\"FOR\"].values.reshape(len(seq_len_group), len(idx_to_label)), cmap='RdPu', annot=True, ax=axes[1,1],\n",
        "            xticklabels=idx_to_label.values(), yticklabels=len_labels)\n",
        "axes[1,1].set_title(\"False Omission Rate For Each Sequence Length and Class\")\n",
        "axes[1,1].set_xlabel(\"Class\")\n",
        "axes[1,1].set_ylabel(\"Sequence Length of Test Examples, x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pof6qqOcNZuj"
      },
      "source": [
        "Use the model to predict the dataset from other source. This is to evaluate the ability of model to generalize well to the unseen expression styles and structures as in the real-world use cases. Apparently, the results shown on the classification report are not encouraging. Not a single metric value exceeds 80% (0.8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n3L_v40EZus"
      },
      "outputs": [],
      "source": [
        "### Test the model using the test data from the external dataset (https://www.kaggle.com/datasets/shivamb/go-emotions-google-emotions-dataset)\n",
        "test_secondData = pd.read_csv('./dataset/val_extData - Copy.csv')\n",
        "label_to_idx = {v: k for k, v in idx_to_label.items()}\n",
        "\n",
        "test_secondData['label_idx'] = 0\n",
        "for label in label_to_idx:\n",
        "  test_secondData.loc[test_secondData[label] == 1, 'label_idx'] = label_to_idx[label]\n",
        "\n",
        "# Lower case 'text' column and remove the punctuations\n",
        "test_secondData['text'] = test_secondData['text'].str.lower().replace('[^\\w\\s]','')\n",
        "\n",
        "## Perform prediction\n",
        "predictions_2 = classification_model.encode(test_secondData['text'])\n",
        "prediction_labels_2 = np.argmax(predictions_2, axis=1)\n",
        "\n",
        "## Print classification report\n",
        "print(classification_report(test_secondData['label_idx'], prediction_labels_2, target_names=list(idx_to_label.values()))) # F1, True Positive,..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pshl_cqi7kzy"
      },
      "source": [
        "## Demostrate outputs with three randomly picked examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTM3lHFY7lTo"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "### Randomly pick three examples from the original dataset\n",
        "examples_test = random.choices(test_data, k=3)\n",
        "\n",
        "print(\"Showing the prediction results for randomly picked test examples:\")\n",
        "print(\"-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\")\n",
        "for test_item in examples_test:\n",
        "    input, label = test_item['text'], test_item['label']\n",
        "    prediction = classification_model.encode(input)\n",
        "\n",
        "    print(\"Input Sentence: \", input)\n",
        "    print(\"Predicted Emotion: \", idx_to_label[np.argmax(prediction)])\n",
        "    print(\"True Emotion: \", idx_to_label[label])\n",
        "    print(\"===================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZBkinK8PUQQ"
      },
      "outputs": [],
      "source": [
        "### Randomly pick three examples from the external dataset\n",
        "examples_test = test_secondData.sample(n=3)\n",
        "\n",
        "print(\"Showing the prediction results for randomly picked test examples:\")\n",
        "print(\"-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\")\n",
        "for index, row in examples_test.iterrows():\n",
        "    input, label = row['text'], row['label_idx']\n",
        "    prediction = classification_model.encode(input)\n",
        "\n",
        "    print(\"Input Sentence: \", input)\n",
        "    print(\"Predicted Emotion: \", idx_to_label[np.argmax(prediction)])\n",
        "    print(\"True Emotion: \", idx_to_label[label])\n",
        "    print(\"===================\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
